{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3444\\1815217456.py:2: DtypeWarning: Columns (27,28,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  kcndic = pd.read_csv('../asset/250327_í•œí•œì¤‘ì‚¬ì „ì„¼í„°.csv')\n"
     ]
    }
   ],
   "source": [
    "naver = pd.read_csv('../asset/250327_í•œí•œì¤‘ì˜¤í”ˆí”„ë¡œ.csv')\n",
    "kcndic = pd.read_csv('../asset/250327_í•œí•œì¤‘ì‚¬ì „ì„¼í„°.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3444\\3154040114.py:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  naver.fillna('', inplace=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3444\\3154040114.py:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  kcndic.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# naver.fillna('', inplace=True)\n",
    "# kcndic.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "naver.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "kcndic.replace(r'^\\s*$', np.nan, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_backup = naver.copy()\n",
    "k_backup = kcndic.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ì—”íŠ¸ë¦¬ID ë§¤ì¹­ë“¤ ì¤‘ ì•ˆì“°ê²Œëœê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chunked_merge(naver, kcndic, chunk_size=50000):\n",
    "#     # âœ… key ìƒì„± (í‘œì œì–´ + ì œê³µì²˜ID)\n",
    "#     naver['key'] = naver['í‘œì œì–´'] + '_' + naver['ì œê³µì²˜ID'].astype(str)\n",
    "#     kcndic['key'] = kcndic['í‘œì œì–´'] + '_' + kcndic['ì œê³µì²˜ID'].astype(str)\n",
    "\n",
    "#     # âœ… naverëŠ” í‘œì œì–´ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°\n",
    "#     naver_sub = naver[['key', 'ì—”íŠ¸ë¦¬ID', 'í¸ì§‘ì', 'ë§ˆì§€ë§‰í¸ì§‘ì‹œê°„', 'í‘œì œì–´ì¢…ë¥˜', 'ë¶€ëª¨í‘œì œì–´ID', 'ì›ì–´ë©¤ë²„ID', 'ë°œìŒID', 'ë³„ëª…ID(ì´í˜•íƒœë¶€)', 'ì˜¤ëŠ˜ì˜ë‹¨ì–´ì„ ì •ì—¬ë¶€', 'ì„œë¹„ìŠ¤ë…¸ì¶œ(í‘œì œì–´ë¶€)', 'ê²€ìƒ‰ë…¸ì¶œ(í‘œì œì–´ë¶€)']].drop_duplicates('key')\n",
    "    \n",
    "#     # ğŸ”„ B (kcndic)ë¥¼ chunk ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì„œ ë³‘í•©\n",
    "#     chunks = []\n",
    "#     total = len(kcndic)\n",
    "    \n",
    "#     for i in range(0, total, chunk_size):\n",
    "#         kcndic_chunk = kcndic.iloc[i:i + chunk_size].copy()\n",
    "#         merged_chunk = pd.merge(kcndic_chunk, naver_sub, on='key', how='left', suffixes=('', '_N'))\n",
    "        \n",
    "#         # ë®ì–´ì“°ê¸°\n",
    "#         for col in naver_sub.columns.difference(['key']):\n",
    "#             merged_chunk[col] = merged_chunk[col + '_N']\n",
    "        \n",
    "#         # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°\n",
    "#         merged_chunk.drop(columns=[col + '_N' for col in naver_sub.columns.difference(['key'])], inplace=True)\n",
    "        \n",
    "#         chunks.append(merged_chunk)\n",
    "#         print(f\"âœ… Chunk {i//chunk_size + 1} ë³‘í•© ì™„ë£Œ\")\n",
    "\n",
    "#     # ğŸ”— ëª¨ë“  chunk í•©ì¹˜ê¸°\n",
    "#     merged = pd.concat(chunks, ignore_index=True)\n",
    "#     print(\"ğŸ‰ ì „ì²´ ë³‘í•© ì™„ë£Œ!\")\n",
    "    \n",
    "#     return merged\n",
    "\n",
    "# result = chunked_merge(naver, kcndic)\n",
    "\n",
    "# result.to_csv('../result/250324_result4_ì—”íŠ¸ë¦¬IDë§Œ_í†µí•©_ì²­í¬ë³„.csv', index=False)\n",
    "# 250324: í•œí•œì¤‘ ì „ì²´ ë½‘ì•„ì™€ì„œ í‘œì œì–´(ì œê³µì²˜ID) ê¸°ì¤€ ì •ë³´ì˜®ê²¨ì˜¤ê¸° ì„±ê³µ\n",
    "# ë‹¤ìŒì—ì˜¤ë©´: ëœ»í’€ì´ ê¸°ì¤€ ì •ë³´ ì˜®ê²¨ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chunked_merge_with_logs(naver, kcndic, chunk_size=50000):\n",
    "\n",
    "#     # âœ… key ìƒì„±\n",
    "#     naver['key'] = naver['í‘œì œì–´'] + '_' + naver['ì œê³µì²˜ID'].astype(str)\n",
    "#     kcndic['key'] = kcndic['í‘œì œì–´'] + '_' + kcndic['ì œê³µì²˜ID'].astype(str)\n",
    "\n",
    "#     # âœ… 1. ì¤‘ë³µëœ key ë¡œê·¸ ì €ì¥\n",
    "#     dup_keys = naver[naver.duplicated('key', keep=False)]\n",
    "#     if not dup_keys.empty:\n",
    "#         dup_keys.to_excel('../log/ì¤‘ë³µ_key_naver.xlsx', index=False)\n",
    "#         print(f\"ğŸŸ¡ naver ì¤‘ë³µ key {len(dup_keys)}ê±´ ì €ì¥ë¨\")\n",
    "\n",
    "#     # âœ… 2. ê³ ìœ  keyë§Œ ì¶”ì¶œ\n",
    "#     naver_sub = naver[['key', 'ì—”íŠ¸ë¦¬ID', 'í¸ì§‘ì', 'ë§ˆì§€ë§‰í¸ì§‘ì‹œê°„', 'í‘œì œì–´ì¢…ë¥˜', 'ë¶€ëª¨í‘œì œì–´ID', 'ì›ì–´ë©¤ë²„ID', \n",
    "#                        'ë°œìŒID', 'ë³„ëª…ID(ì´í˜•íƒœë¶€)', 'ì˜¤ëŠ˜ì˜ë‹¨ì–´ì„ ì •ì—¬ë¶€', 'ì„œë¹„ìŠ¤ë…¸ì¶œ(í‘œì œì–´ë¶€)', 'ê²€ìƒ‰ë…¸ì¶œ(í‘œì œì–´ë¶€)']\n",
    "#                      ].drop_duplicates('key')\n",
    "\n",
    "#     chunks = []\n",
    "#     unmatched_rows = []\n",
    "\n",
    "#     total = len(kcndic)\n",
    "#     for i in range(0, total, chunk_size):\n",
    "#         kcndic_chunk = kcndic.iloc[i:i + chunk_size].copy()\n",
    "#         merged_chunk = pd.merge(kcndic_chunk, naver_sub, on='key', how='left', suffixes=('', '_N'))\n",
    "\n",
    "#         # ë®ì–´ì“°ê¸°\n",
    "#         for col in naver_sub.columns.difference(['key']):\n",
    "#             merged_chunk[col] = merged_chunk[col + '_N']\n",
    "#         merged_chunk.drop(columns=[col + '_N' for col in naver_sub.columns.difference(['key'])], inplace=True)\n",
    "\n",
    "#         # âœ… ë³‘í•© ì‹¤íŒ¨í•œ ì—”íŠ¸ë¦¬ID ì—†ëŠ” row ìˆ˜ì§‘\n",
    "#         unmatched = merged_chunk[merged_chunk['ì—”íŠ¸ë¦¬ID'].isna()]\n",
    "#         if not unmatched.empty:\n",
    "#             unmatched_rows.append(unmatched[['í‘œì œì–´', 'ì œê³µì²˜ID']].drop_duplicates())\n",
    "\n",
    "#         chunks.append(merged_chunk)\n",
    "#         print(f\"âœ… Chunk {i//chunk_size + 1} ë³‘í•© ì™„ë£Œ\")\n",
    "\n",
    "#     # ì „ì²´ ë³‘í•©\n",
    "#     merged = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "#     # âœ… ë§¤ì¹­ ì•ˆëœ kcndic ë¡œê·¸ ì €ì¥\n",
    "#     if unmatched_rows:\n",
    "#         unmatched_all = pd.concat(unmatched_rows).drop_duplicates()\n",
    "#         unmatched_all.to_excel('../log/ì—”íŠ¸ë¦¬ID_ë§¤ì¹­ì‹¤íŒ¨_kcndic.xlsx', index=False)\n",
    "#         print(f\"ğŸŸ¥ ë§¤ì¹­ ì‹¤íŒ¨ kcndic í‘œì œì–´ {len(unmatched_all)}ê±´ ë¡œê·¸ ì €ì¥ë¨\")\n",
    "\n",
    "#     print(\"ğŸ‰ ì „ì²´ ë³‘í•© ì™„ë£Œ!\")\n",
    "#     return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ê°€ì¥ ë¹ ë¥´ê³  ì˜¤ë¥˜ì—†ëŠ”ë“¯! \n",
    "# def safe_chunked_merge_with_logs2(naver, kcndic, chunk_size=50000):\n",
    "#     ### ğŸ”‘ key ìƒì„± (ìœ íš¨í•œ í–‰ë§Œ)\n",
    "#     def make_key(df, source):\n",
    "#         df['key'] = None\n",
    "#         valid_mask = (\n",
    "#             df['í‘œì œì–´'].notna() & (df['í‘œì œì–´'].astype(str).str.strip() != '') &\n",
    "#             df['ì œê³µì²˜ID'].notna() & (df['ì œê³µì²˜ID'].astype(str).str.strip() != '')\n",
    "#         )\n",
    "#         # valid_mask = (df['í‘œì œì–´'] != '') & (df['ì œê³µì²˜ID'] != '')\n",
    "#         df.loc[valid_mask, 'key'] = df.loc[valid_mask, 'í‘œì œì–´'] + '_' + df.loc[valid_mask, 'ì œê³µì²˜ID'].astype(str)\n",
    "\n",
    "#         # ğŸŸ¡ ë¶ˆì™„ì „ key ë¡œê·¸ ì €ì¥ (í‘œì œì–´ë‚˜ ì œê³µì²˜ID ì¤‘ í•˜ë‚˜ë§Œ ë¹„ì–´ ìˆìŒ)\n",
    "#         partial_null_mask = (\n",
    "#             ((df['í‘œì œì–´'].isna()) | (df['í‘œì œì–´'].astype(str).str.strip() == '')) &\n",
    "#             ((df['ì œê³µì²˜ID'].notna()) & (df['ì œê³µì²˜ID'].astype(str).str.strip() != ''))\n",
    "#         ) | (\n",
    "#             ((df['í‘œì œì–´'].notna()) & (df['í‘œì œì–´'].astype(str).str.strip() != '')) &\n",
    "#             ((df['ì œê³µì²˜ID'].isna()) | (df['ì œê³µì²˜ID'].astype(str).str.strip() == ''))\n",
    "#         )\n",
    "#         # partial_null_mask = (\n",
    "#         #     (df['í‘œì œì–´'] == '') & (df['ì œê³µì²˜ID'] != '')\n",
    "#         # ) | (\n",
    "#         #     (df['í‘œì œì–´'] != '') & (df['ì œê³µì²˜ID'] == '')\n",
    "#         # )\n",
    "#         partial_nulls = df[partial_null_mask]\n",
    "#         if not partial_nulls.empty:\n",
    "#             partial_nulls.to_excel(f\"../log/250000_result0_{source}_ë¶ˆì™„ì „_key_ë¡œê·¸.xlsx\", index=False)\n",
    "#             print(f\"ğŸŸ  {source} ë‚´ ë¶ˆì™„ì „ key {len(partial_nulls)}ê±´ ë¡œê·¸ ì €ì¥ë¨\")\n",
    "\n",
    "#         return df\n",
    "\n",
    "#     # 1ï¸âƒ£ í‚¤ ìƒì„±\n",
    "#     naver = make_key(naver, 'naver')\n",
    "#     kcndic = make_key(kcndic, 'kcndic')\n",
    "\n",
    "#     # 2ï¸âƒ£ ì¤‘ë³µ key ë¡œê·¸ ì €ì¥ (naver only)\n",
    "#     dup_keys = naver[naver.duplicated('key', keep=False) & naver['key'].notna()]\n",
    "#     if not dup_keys.empty:\n",
    "#         dup_keys.to_excel('../log/250000_result0_ì¤‘ë³µ_key_naver.xlsx', index=False)\n",
    "#         print(f\"ğŸŸ¡ naver ì¤‘ë³µ key {len(dup_keys)}ê±´ ë¡œê·¸ ì €ì¥ë¨\")\n",
    "\n",
    "#     # 3ï¸âƒ£ ê³ ìœ  keyë§Œ ì¶”ì¶œ\n",
    "#     headword_cols = [\n",
    "#         'ë²ˆí˜¸', 'í‘œì œì–´ë²ˆí˜¸', 'ì—”íŠ¸ë¦¬ID', 'ì§‘í•„ìƒíƒœ', 'í¸ì§‘ì', \n",
    "#         'ìµœì´ˆë“±ë¡ì‹œê°„', 'ë§ˆì§€ë§‰í¸ì§‘ì‹œê°„', 'í‘œì œì–´ì¢…ë¥˜', \n",
    "#         'ë¶€ëª¨í‘œì œì–´ID', 'ì›ì–´ë©¤ë²„ID', \n",
    "#         'ë°œìŒID', 'ë°œìŒì¢…ë¥˜', 'ì—¬ì„±ë°œìŒíŒŒì¼ê²½ë¡œ', \n",
    "#         'ë³„ëª…ID(ì´í˜•íƒœë¶€)', 'ì˜¤ëŠ˜ì˜ë‹¨ì–´ì„ ì •ì—¬ë¶€', 'ì„œë¹„ìŠ¤ë…¸ì¶œ(í‘œì œì–´ë¶€)', 'ê²€ìƒ‰ë…¸ì¶œ(í‘œì œì–´ë¶€)'\n",
    "#     ]\n",
    "#     # naver_sub = naver[['key'] + headword_cols].drop_duplicates('key')\n",
    "#     naver_sub = naver[naver['key'].notna()][['key'] + headword_cols].drop_duplicates('key')\n",
    "\n",
    "    \n",
    "#     # 4ï¸âƒ£ chunk ë³‘í•©\n",
    "#     chunks = []\n",
    "#     unmatched_rows = []\n",
    "#     total = len(kcndic)\n",
    "\n",
    "#     for i in range(0, total, chunk_size):\n",
    "#         kc_chunk = kcndic.iloc[i:i+chunk_size].copy()\n",
    "\n",
    "#         merged_chunk = pd.merge(kc_chunk, naver_sub, on='key', how='left', suffixes=('', '_N'))\n",
    "\n",
    "#         for col in headword_cols:\n",
    "#             merged_chunk[col] = merged_chunk[col + '_N']\n",
    "        \n",
    "#         merged_chunk.drop(columns=[col + '_N' for col in headword_cols], inplace=True)\n",
    "\n",
    "#         # ë§¤ì¹­ ì‹¤íŒ¨ ì—”íŠ¸ë¦¬ID ë¡œê·¸\n",
    "#         unmatched = merged_chunk[merged_chunk['ì—”íŠ¸ë¦¬ID'].isna() & merged_chunk['key'].notna()]\n",
    "        \n",
    "#         if not unmatched.empty:\n",
    "#             unmatched_rows.append(unmatched[['í‘œì œì–´', 'ì œê³µì²˜ID']].drop_duplicates())\n",
    "\n",
    "#         chunks.append(merged_chunk)\n",
    "#         print(f\"âœ… Chunk {i//chunk_size + 1} ë³‘í•© ì™„ë£Œ\")\n",
    "\n",
    "#     # 5ï¸âƒ£ ì „ì²´ ë³‘í•© ê²°ê³¼\n",
    "#     merged = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "#     # 6ï¸âƒ£ ë§¤ì¹­ ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥\n",
    "#     if unmatched_rows:\n",
    "#         unmatched_all = pd.concat(unmatched_rows).drop_duplicates()\n",
    "#         unmatched_all.to_excel('../log/250000_result0_ì—”íŠ¸ë¦¬ID_ë§¤ì¹­ì‹¤íŒ¨_kcndic.xlsx', index=False)\n",
    "#         print(f\"ğŸŸ¥ ì—”íŠ¸ë¦¬ID ë§¤ì¹­ ì‹¤íŒ¨ {len(unmatched_all)}ê±´ ë¡œê·¸ ì €ì¥ë¨\")\n",
    "\n",
    "#     print(\"ğŸ‰ ì „ì²´ ë³‘í•© ì™„ë£Œ!\")\n",
    "#     return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ  naver ë‚´ ë¶ˆì™„ì „ key 1311ê±´ ë¡œê·¸ ì €ì¥ë¨\n",
      "âœ… Chunk 1 ë³‘í•© ì™„ë£Œ\n",
      "âœ… Chunk 2 ë³‘í•© ì™„ë£Œ\n",
      "ğŸŸ¥ ì—”íŠ¸ë¦¬ID ë§¤ì¹­ ì‹¤íŒ¨ 5044ê±´ ë¡œê·¸ ì €ì¥ë¨\n",
      "ğŸ‰ ì „ì²´ ë³‘í•© ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# result2 = safe_chunked_merge_with_logs2(naver, kcndic)\n",
    "# # ì—­ì‹œ ê°€ì¥ ê¹”ë”í•œë“¯? 3ì´ˆë¼ë‹ˆ..\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ì—”íŠ¸ë¦¬ID ë§¤ì¹­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã„±ã„±\n",
    "def entryId_merge(naver, kcndic, chunk_size=50000):\n",
    "    ### ğŸ”‘ key ìƒì„± (ìœ íš¨í•œ í–‰ë§Œ)\n",
    "    def make_key(df, source):\n",
    "        df['key'] = None\n",
    "\n",
    "        # ìœ íš¨í•œ í–‰: [í‘œì œì–´], [ì œê³µì²˜ID] ëª¨ë‘ NaNì•„ë‹˜\n",
    "        valid_mask = df['í‘œì œì–´'].notna() & df['ì œê³µì²˜ID'].notna()\n",
    "        df.loc[valid_mask, 'key'] = df.loc[valid_mask, 'í‘œì œì–´'] + '_' + df.loc[valid_mask, 'ì œê³µì²˜ID'].astype(str)\n",
    "\n",
    "        # ğŸŸ¡ ë¶ˆì™„ì „ key ë¡œê·¸ ì €ì¥ (í‘œì œì–´ë‚˜ ì œê³µì²˜ID ì¤‘ í•˜ë‚˜ë§Œ ë¹„ì–´ ìˆìŒ)\n",
    "        invalid_mask = (\n",
    "            df['í‘œì œì–´'].isna() & df['ì œê³µì²˜ID'].notna() |\n",
    "            df['í‘œì œì–´'].notna() & df['ì œê³µì²˜ID'].isna()\n",
    "        )\n",
    "        invalid_rows = df[invalid_mask]\n",
    "        if not invalid_rows.empty:\n",
    "            # invalid_rows.to_excel(f\"../log/250000_result0_{source}_ë¶ˆì™„ì „_key_ë¡œê·¸.xlsx\", index=False)\n",
    "            print(f\"ğŸŸ  {source} ë‚´ ë§¤ì¹­ë¶ˆê°€ key {len(invalid_rows)}ê±´ ë¡œê·¸ ì €ì¥ë¨\")\n",
    "        return df\n",
    "\n",
    "    # 1ï¸âƒ£ í‚¤ ìƒì„±\n",
    "    naver = make_key(naver, 'naver')\n",
    "    kcndic = make_key(kcndic, 'kcndic')\n",
    "\n",
    "    # 2ï¸âƒ£ ì¤‘ë³µ key ë¡œê·¸ ì €ì¥ (naver only)\n",
    "    dup_keys = naver[naver.duplicated('key', keep=False) & naver['key'].notna()]\n",
    "    if not dup_keys.empty:\n",
    "        # dup_keys.to_excel('../log/250000_result0_ì¤‘ë³µ_key_naver.xlsx', index=False)\n",
    "        print(f\"ğŸŸ¡ naver ì¤‘ë³µ key {len(dup_keys)}ê±´ ë¡œê·¸ ì €ì¥ë¨\")\n",
    "\n",
    "    # 3ï¸âƒ£ ê³ ìœ  keyë§Œ ì¶”ì¶œ\n",
    "    headword_cols = [\n",
    "        'ë²ˆí˜¸', 'í‘œì œì–´ë²ˆí˜¸', 'ì—”íŠ¸ë¦¬ID', 'ì§‘í•„ìƒíƒœ', 'í¸ì§‘ì', \n",
    "        'ìµœì´ˆë“±ë¡ì‹œê°„', 'ë§ˆì§€ë§‰í¸ì§‘ì‹œê°„', 'í‘œì œì–´ì¢…ë¥˜', \n",
    "        'ë¶€ëª¨í‘œì œì–´ID', 'ì›ì–´ë©¤ë²„ID', \n",
    "        'ë°œìŒID', 'ë°œìŒì¢…ë¥˜', 'ì—¬ì„±ë°œìŒíŒŒì¼ê²½ë¡œ', \n",
    "        'ë³„ëª…ID(ì´í˜•íƒœë¶€)', 'ì¢…ë¥˜(ì´í˜•íƒœë¶€)', 'ì˜¤ëŠ˜ì˜ë‹¨ì–´ì„ ì •ì—¬ë¶€', 'ì„œë¹„ìŠ¤ë…¸ì¶œ(í‘œì œì–´ë¶€)', 'ê²€ìƒ‰ë…¸ì¶œ(í‘œì œì–´ë¶€)'\n",
    "    ]\n",
    "    naver_sub = naver[naver['key'].notna()][['key'] + headword_cols].drop_duplicates('key')\n",
    "\n",
    "    # 4ï¸âƒ£ chunk ë³‘í•©\n",
    "    chunks = []\n",
    "    unmatched_rows = []\n",
    "    total = len(kcndic)\n",
    "\n",
    "    for i in range(0, total, chunk_size):\n",
    "        kc_chunk = kcndic.iloc[i:i+chunk_size].copy()\n",
    "\n",
    "        merged_chunk = pd.merge(kc_chunk, naver_sub, on='key', how='left', suffixes=('', '_N'))\n",
    "\n",
    "        for col in headword_cols:\n",
    "            merged_chunk[col] = merged_chunk[col + '_N']\n",
    "\n",
    "        merged_chunk.drop(columns=[col + '_N' for col in headword_cols], inplace=True)\n",
    "\n",
    "        # ë§¤ì¹­ ì‹¤íŒ¨ ì—”íŠ¸ë¦¬ID ë¡œê·¸ => kcndic í‘œì œì–´ ì¤‘ ì—”íŠ¸ë¦¬ID ëª»ë°›ì€ ê²ƒë“¤ë“¤\n",
    "        unmatched = merged_chunk[merged_chunk['ì—”íŠ¸ë¦¬ID'].isna() & merged_chunk['key'].notna()]\n",
    "        \n",
    "        if not unmatched.empty:\n",
    "            unmatched_rows.append(unmatched[['í‘œì œì–´', 'ì œê³µì²˜ID']].drop_duplicates())\n",
    "\n",
    "        chunks.append(merged_chunk)\n",
    "        print(f\"âœ… Chunk {i//chunk_size + 1} ë³‘í•© ì™„ë£Œ\")\n",
    "\n",
    "    # 5ï¸âƒ£ ì „ì²´ ë³‘í•© ê²°ê³¼\n",
    "    merged = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "    # 6ï¸âƒ£ ë§¤ì¹­ ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥\n",
    "    if unmatched_rows:\n",
    "        unmatched_all = pd.concat(unmatched_rows).drop_duplicates()\n",
    "        # unmatched_all.to_excel('../log/250000_result0_ì—”íŠ¸ë¦¬ID_ë§¤ì¹­ì‹¤íŒ¨_kcndic.xlsx', index=False)\n",
    "        print(f\"ğŸŸ¥ ì—”íŠ¸ë¦¬ID ë§¤ì¹­ ì‹¤íŒ¨ {len(unmatched_all)}ê±´ ë¡œê·¸ ì €ì¥ë¨\")\n",
    "\n",
    "    print(\"ğŸ‰ ì „ì²´ ë³‘í•© ì™„ë£Œ!\")\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ  naver ë‚´ ë§¤ì¹­ë¶ˆê°€ key 1311ê±´ ë¡œê·¸ ì €ì¥ë¨\n",
      "âœ… Chunk 1 ë³‘í•© ì™„ë£Œ\n",
      "âœ… Chunk 2 ë³‘í•© ì™„ë£Œ\n",
      "ğŸŸ¥ ì—”íŠ¸ë¦¬ID ë§¤ì¹­ ì‹¤íŒ¨ 5044ê±´ ë¡œê·¸ ì €ì¥ë¨\n",
      "ğŸ‰ ì „ì²´ ë³‘í•© ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ã„±ã„±\n",
    "result = entryId_merge(naver, kcndic)\n",
    "# ì•¼í˜¸!! ê¹”ë”í•˜ê³  ë¹ ë¥´ë‹¤. ì´ê±° ì±„íƒã„±ã„± \n",
    "# naver 1311ê±´ => 1000ê°œëŠ” ì „ì„ìê°€ ì œê³µì²˜ID ì•ˆë„£ìŒ. 300ê°œëŠ” ì‚­ì œí•´ì•¼í•˜ëŠ” ë°ì´í„°....?ì¸ê±´ê°€? \n",
    "# kcndic 5044ê±´ => 1000ê°œëŠ” naverì—ì„œ ë¶ˆì™„ì „ keyë¼ ëª»ë¶™ìŒ. 4000ê°œì •ë„ëŠ” ì‹ ê·œì–´ë¼ naverì— ì• ì´ˆì— ì—†ì—ˆìŒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã„±ã„± \n",
    "r_backup = result.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã„±ã„±\n",
    "def entryID_retry_origin(naver, result):\n",
    "    \"\"\"\n",
    "    1ì°¨ ë³‘í•© ì‹¤íŒ¨ ì¤‘ ì œê³µì²˜ID ì—†ì–´ì„œ ì‹¤íŒ¨í•œ ì• ë“¤ì„\n",
    "    í‘œì œì–´ + ì›ì–´ ê¸°ë°˜ìœ¼ë¡œ ì—”íŠ¸ë¦¬ID ì¬ë³‘í•© (retry_key)\n",
    "\n",
    "    âœ… ê¸°ì¡´ ì—”íŠ¸ë¦¬ID ë“± ìœ ì§€\n",
    "    âœ… retry ì„±ê³µí•œ í–‰ë§Œ ì •í™•í•˜ê²Œ resultì— ë³‘í•©\n",
    "    âœ… index ì˜ì¡´ âŒ, retry_key ê¸°ë°˜ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì—…ë°ì´íŠ¸\n",
    "    \"\"\"\n",
    "\n",
    "    # ğŸ”‘ ë³‘í•© ëŒ€ìƒ: naver ì¤‘ ì œê³µì²˜ID ì—†ì–´ key ëª» ë§Œë“  ì• ë“¤\n",
    "    naver_failed = naver[naver['key'].isna() & naver['í‘œì œì–´'].notna()].copy()\n",
    "    print(f\"ğŸŸ  ì œê³µì²˜ID ì—†ì–´ ë³‘í•© ì‹¤íŒ¨í•œ naver í–‰ ìˆ˜: {len(naver_failed)}\")\n",
    "\n",
    "    # retry_key ìƒì„±\n",
    "    naver_failed['retry_key'] = naver_failed['í‘œì œì–´']\n",
    "    with_origin = naver_failed['ì›ì–´'].notna()\n",
    "    naver_failed.loc[with_origin, 'retry_key'] = (\n",
    "        naver_failed.loc[with_origin, 'í‘œì œì–´'] + '_' + naver_failed.loc[with_origin, 'ì›ì–´'].astype(str)\n",
    "    )\n",
    "\n",
    "    # ğŸ” ë³‘í•© ëŒ€ìƒ ì¶”ì¶œ\n",
    "    headword_cols = [\n",
    "        'ë²ˆí˜¸', 'í‘œì œì–´ë²ˆí˜¸', 'ì—”íŠ¸ë¦¬ID', 'ì§‘í•„ìƒíƒœ', 'í¸ì§‘ì', \n",
    "        'ìµœì´ˆë“±ë¡ì‹œê°„', 'ë§ˆì§€ë§‰í¸ì§‘ì‹œê°„', 'í‘œì œì–´ì¢…ë¥˜', \n",
    "        'ë¶€ëª¨í‘œì œì–´ID', 'ì›ì–´ë©¤ë²„ID', \n",
    "        'ë°œìŒID', 'ë°œìŒì¢…ë¥˜', 'ì—¬ì„±ë°œìŒíŒŒì¼ê²½ë¡œ', \n",
    "        'ë³„ëª…ID(ì´í˜•íƒœë¶€)', 'ì¢…ë¥˜(ì´í˜•íƒœë¶€)', 'ì˜¤ëŠ˜ì˜ë‹¨ì–´ì„ ì •ì—¬ë¶€', 'ì„œë¹„ìŠ¤ë…¸ì¶œ(í‘œì œì–´ë¶€)', 'ê²€ìƒ‰ë…¸ì¶œ(í‘œì œì–´ë¶€)'\n",
    "    ]\n",
    "\n",
    "    before_dedup = len(naver_failed)\n",
    "\n",
    "    # ğŸ” ì¤‘ë³µ ì œì™¸ëœ í–‰ ì¶”ì¶œ\n",
    "    duplicated_keys = naver_failed[naver_failed['retry_key'].notna()]['retry_key']\n",
    "    duplicated_only = naver_failed[naver_failed['retry_key'].isin(duplicated_keys[duplicated_keys.duplicated()])]\n",
    "    print(\"ğŸ” ì¤‘ë³µëœ retry_keyë¡œ ì¸í•´ ì œì™¸ëœ í–‰ë“¤:\")\n",
    "    print(duplicated_only[['retry_key', 'í‘œì œì–´', 'ì›ì–´', 'ì—”íŠ¸ë¦¬ID']])  # ì¼ë¶€ë§Œ ë³´ê¸°\n",
    "\n",
    "\n",
    "    naver_sub = naver_failed[naver_failed['retry_key'].notna()][['retry_key'] + headword_cols].drop_duplicates('retry_key')\n",
    "    after_dedup = len(naver_sub)\n",
    "    print(f\"ğŸŸ¡ retry_key ì¤‘ë³µìœ¼ë¡œ ì œì™¸ëœ í–‰ ìˆ˜: {before_dedup - after_dedup}\")\n",
    "\n",
    "    # ğŸ“Œ result ë‚´ retry ëŒ€ìƒ ìƒì„±\n",
    "    result = result.copy()  # ì›ë³¸ ë³´í˜¸\n",
    "    result['retry_key'] = None\n",
    "    with_origin = result['ì›ì–´'].notna()\n",
    "    result.loc[result['í‘œì œì–´'].notna(), 'retry_key'] = result['í‘œì œì–´']\n",
    "    result.loc[with_origin, 'retry_key'] = (\n",
    "        result.loc[with_origin, 'í‘œì œì–´'] + '_' + result.loc[with_origin, 'ì›ì–´'].astype(str)\n",
    "    )\n",
    "\n",
    "    retry_target = result[\n",
    "        result['key'].notna() & result['ì—”íŠ¸ë¦¬ID'].isna() & result['retry_key'].notna()\n",
    "    ][['retry_key']].drop_duplicates()\n",
    "\n",
    "    # ë³‘í•© ì‹œë„\n",
    "    matched = pd.merge(\n",
    "        retry_target,\n",
    "        naver_sub,\n",
    "        on='retry_key',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # ì„±ê³µ / ì‹¤íŒ¨ ë¶„ë¦¬\n",
    "    matched_success = matched[matched['ì—”íŠ¸ë¦¬ID'].notna()].copy()\n",
    "    unmatched = matched[matched['ì—”íŠ¸ë¦¬ID'].isna()].copy()\n",
    "\n",
    "    # âœ… ë®ì–´ì“°ê¸° (retry_key ê¸°ì¤€ merge-update)\n",
    "    for col in headword_cols:\n",
    "        result = result.merge(\n",
    "            matched_success[['retry_key', col]],\n",
    "            on='retry_key',\n",
    "            how='left',\n",
    "            suffixes=('', '_new')\n",
    "        )\n",
    "        result[col] = result[col].combine_first(result[col + '_new'])\n",
    "        result.drop(columns=[col + '_new'], inplace=True)\n",
    "\n",
    "    # âœ… ë¡œê·¸ ì €ì¥\n",
    "    if not matched_success.empty:\n",
    "        matched_success[['retry_key']].to_excel('../log/retry_matched.xlsx', index=False)\n",
    "        print(f\"ğŸŸ¢ retry ë³‘í•© ì„±ê³µ: {len(matched_success)}ê±´ â†’ retry_matched.xlsx ì €ì¥ë¨\")\n",
    "\n",
    "    if not unmatched.empty:\n",
    "        unmatched[['retry_key']].to_excel('../log/retry_unmatched.xlsx', index=False)\n",
    "        print(f\"ğŸ”´ retry ë³‘í•© ì‹¤íŒ¨: {len(unmatched)}ê±´ â†’ retry_unmatched.xlsx ì €ì¥ë¨\")\n",
    "\n",
    "    print(f\"ğŸ” retry ë³‘í•© ì™„ë£Œ: ì´ {len(matched_success)}ê±´ ì—”íŠ¸ë¦¬ID ë³´ê°•\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ  ì œê³µì²˜ID ì—†ì–´ ë³‘í•© ì‹¤íŒ¨í•œ naver í–‰ ìˆ˜: 1311\n",
      "ğŸ” ì¤‘ë³µëœ retry_keyë¡œ ì¸í•´ ì œì™¸ëœ í–‰ë“¤:\n",
      "           retry_key   í‘œì œì–´       ì›ì–´                             ì—”íŠ¸ë¦¬ID\n",
      "100847           -ë¼ë©°   -ë¼ë©°      NaN  ff24baf2032c4894a8df69019ed656b7\n",
      "109780           ë˜ë¥´ë¥´   ë˜ë¥´ë¥´      NaN  3eb9bac315724438b2824602d89b621b\n",
      "115798           ë˜ë¥´ë¥´   ë˜ë¥´ë¥´      NaN  ba7f86bcbe9f4f9f998f6b44b9c29091\n",
      "116851           -ë¼ë©°   -ë¼ë©°      NaN  4bd4034fe8344d35a2180f86811a71c7\n",
      "129749           -ê¸°ë‹¤   -ê¸°ë‹¤      NaN  dee2e31ce16f44fba7a5d0f3fa3fccda\n",
      "130199           -ê¸°ë‹¤   -ê¸°ë‹¤      NaN  2b8fe3640f934fb5a7a6f7b26e65965e\n",
      "135027  íŠ€ë¥´í‚¤ì˜ˆ_TÃ¼rkiye  íŠ€ë¥´í‚¤ì˜ˆ  TÃ¼rkiye  0a5d4fcb947245d3910274d91208def0\n",
      "138066  íŠ€ë¥´í‚¤ì˜ˆ_TÃ¼rkiye  íŠ€ë¥´í‚¤ì˜ˆ  TÃ¼rkiye  8290805ce54b4fda88271fbda2209f87\n",
      "ğŸŸ¡ retry_key ì¤‘ë³µìœ¼ë¡œ ì œì™¸ëœ í–‰ ìˆ˜: 4\n",
      "ğŸŸ¢ retry ë³‘í•© ì„±ê³µ: 983ê±´ â†’ retry_matched.xlsx ì €ì¥ë¨\n",
      "ğŸ”´ retry ë³‘í•© ì‹¤íŒ¨: 4043ê±´ â†’ retry_unmatched.xlsx ì €ì¥ë¨\n",
      "ğŸ” retry ë³‘í•© ì™„ë£Œ: ì´ 983ê±´ ì—”íŠ¸ë¦¬ID ë³´ê°•\n"
     ]
    }
   ],
   "source": [
    "# ã„±ã„±\n",
    "result2 = entryID_retry_origin(naver, result)  # ì œê³µì²˜ID ì—†ëŠ” ì• ë“¤ retry\n",
    "result2.to_csv('../result/250330_id_result1_ì—”íŠ¸ë¦¬ID_ì œê³µì²˜IDì—†ë˜ì• ë“¤ì‹œë„.csv', index=False)\n",
    "\n",
    "# 250328: ì™€ ì´ë°©ë²•ì´ ë˜ë„¤.... ã…œã…œã…œ ì½”ë“œ í•œë²ˆ ì ê²€í•´ë³´ì ë¬¸ì œ ì—†ëŠ”ì§€........ í•˜ì´ì œì•¼ì§„ì§œ ëœ»í’€ì´ì— ë“¤ì–´ê°ˆìˆ˜ ìˆìŒ...!!! \n",
    "# 250330: ì•„ë˜ cellì— í”„ë¦°íŠ¸ëœ, ì¤‘ë³µëœ retry_keyë¡œ ì¸í•´ ì œì™¸ëœ í–‰ë“¤ì˜ê²½ìš°? \n",
    "#       - ê·¸ëƒ¥ ë‚˜ì¤‘ì— ìˆ˜ë™ìœ¼ë¡œ ë„£ì..ã…ã….. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ëœ»í’€ì´ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target : ê¸°ì¡´\n",
    "\n",
    "kcndic : 250327_id_result2.1_ì˜ëª»ëœì „ì²˜ë¦¬ìˆ˜ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. ì—”íŠ¸ë¦¬IDë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•œ í‘œì œì–´ëŠ” ë‹¹ì—°íˆ ê·¸ ì™¸ IDë„ ì¡´ì¬í•˜ì§€ ì•Šì„ê²ƒì´ë¯€ë¡œ ì œì™¸ í•„ìš” (ì—”íŠ¸ë¦¬ ë§¤ì¹­ì€ í• ë§Œí¼í–ˆìœ¼ë‹ˆ ì´ì œ ì—†ëŠ”ê±´ ì¿¨í•˜ê²Œ ë¹¼ë²„ë¦¬ì!) \n",
    "    ì—”íŠ¸ë¦¬IDì— ì¢…ì†ë˜ëŠ” êµ¬ì¡°ë¥¼? ì–´ì¼€ ì²˜ë¦¬í• ì§€ë„ ìƒê°ì¢€.. \n",
    "2. í‘œì œì–´ì¢…ë¥˜: ë‹¨ì–´, ì†ë‹´, ìˆ™ì–´ ë¶„ë¦¬í• ì§€ ì •í•´ì•¼í•¨\n",
    "3. ì‚¬ì‹¤ì€ê± ë­... ì—”íŠ¸ë¦¬IDë‘ ëœ»í’€ì´(ì²˜ìŒnê¸€ìí•œì •) ë³‘í•©í‚¤ í•´ë²„ë¦¬ë©´ ì œì¼ í¸í• í…ë°... ìš”ëª‡ë…„ì‚¬ì´ ëœ»í’€ì´ ìˆ˜ì •ëœê²ƒë„ ë§ì„ê±°ê°™ë‹¨ë§ì´ì•¼ í•˜ ì´ê±° ì–´ì¼€ ë§¤ì¹­í•¨;; \n",
    "4. ëœ»í’€ì´...... ì—ì„œ ë­ a hrefì²˜ë¦¬ê°™ì€ê±° í•´ì•¼í•˜ëŠ”ë°? ì´ê±´ ë˜ ì–´ì¼€í•¨? ì‚°ë„˜ì–´ì‚°..\n",
    "    naver: {\"ko\":\"í˜•ìš©ì‚¬ â€˜<a href=\\\"#/entry/kozh/89a2b79ee9304fdf9ee2948b43650933\\\">ì„œê¸€í”„ë‹¤</a>â€™ì˜ í™œìš©í˜•. í•´ì²´ì˜ ì˜ë¬¸í˜•, ê°íƒ„í˜• ë“±ìœ¼ë¡œ ì“°ì¸ë‹¤.\"}\n",
    "    kcndic: {\"ko\":\"í˜•ìš©ì‚¬ â€˜ì„œê¸€í”„ë‹¤&EID=172456;â€™ì˜ í™œìš©í˜•. í•´ì²´ì˜ ì˜ë¬¸í˜•, ê°íƒ„í˜• ë“±ìœ¼ë¡œ ì“°ì¸ë‹¤.\"}\n",
    "    => ì „ì²˜ë¦¬ í•„ìš”. íŠ¹ë³„íˆ ì–´ë µì§„ ì•Šì„ë“¯. ê·¼ë° ì—”íŠ¸ë¦¬ID ë§¤ì¹­ ì•ˆëœ ì• ë©´ ì–´ìº„? \n",
    "    => ì „ì²˜ë¦¬ ì–´ë ¤ìš¸ì‹œ ì¼ë‹¨ ì„œë¡œ ë‹¤ë¥¸ ë¶€ìœ„ëŠ” ë¹¼ë²„ë¦¬ê³  í‚¤ë¡œ ë§Œë“¤ì.. \n",
    "\n",
    "5. ì•„ë¬´íŠ¼ ë‚´ê°€ ì“¸ìˆ˜ìˆëŠ”ê²ƒ:\n",
    "    í‘œì œì–´ìª½ì •ë³´, í’ˆì‚¬, ëœ»í’€ì´, ì„œìˆ í˜•ëœ»í’€ì´, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã„±ã„±ã„±\n",
    "r2_backup = result2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã„±ã„±\n",
    "# naver, kcndic ëœ»í’€ì´ ì „ì²˜ë¦¬..\n",
    "def clean_naver_definition(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # return re.sub(r'<a [^>]*?>.*?</a>', '', text)\n",
    "    return re.sub(r'<a href=\\\\\"[^>]+\\\\\"?>(.*?)</a>', r'\\1', text)\n",
    "\n",
    "def clean_kcndic_definition(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return re.sub(r'&EID=\\d+;', '', text)\n",
    "\n",
    "def safe_fill_entry_id(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    entry_filled = df['ì—”íŠ¸ë¦¬ID'].ffill()\n",
    "    word_filled = df['í‘œì œì–´'].ffill()\n",
    "    fill_mask = df['í‘œì œì–´'].isna() & df['ì—”íŠ¸ë¦¬ID'].isna()\n",
    "    valid_headwords = df.loc[df['í‘œì œì–´'].notna() & df['ì—”íŠ¸ë¦¬ID'].notna(), ['í‘œì œì–´', 'ì—”íŠ¸ë¦¬ID']]\n",
    "    headword_to_entry = valid_headwords.drop_duplicates('í‘œì œì–´').set_index('í‘œì œì–´')['ì—”íŠ¸ë¦¬ID'].to_dict()\n",
    "    fill_mask &= word_filled.map(headword_to_entry).notna()\n",
    "    df['ì—”íŠ¸ë¦¬ID_f'] = df['ì—”íŠ¸ë¦¬ID']\n",
    "    df.loc[fill_mask, 'ì—”íŠ¸ë¦¬ID_f'] = entry_filled[fill_mask]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã„±ã„±\n",
    "# âœ… ì•ˆì „í•˜ê²Œ ì—”íŠ¸ë¦¬ID ì±„ìš°ê¸°\n",
    "naver = safe_fill_entry_id(naver)  \n",
    "result2 = safe_fill_entry_id(result2)\n",
    "\n",
    "\n",
    "# ğŸ” ë¡œê·¸ë¡œ ì €ì¥\n",
    "naver[['ì—”íŠ¸ë¦¬ID_f', 'í‘œì œì–´', 'í‘œì œì–´ì¢…ë¥˜']].to_excel('../log/250331_naver_entry_fill_.xlsx', index=False)\n",
    "result2[['ì—”íŠ¸ë¦¬ID_f', 'í‘œì œì–´', 'í‘œì œì–´ì¢…ë¥˜']].to_excel('../log/250331_kcndic_entry_fill_.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã„´ã„´\n",
    "def meanId_merge_by_entryID_meaning(naver_df, kcndic_df):\n",
    "    \"\"\"\n",
    "    [í‘œì œì–´] ê¸°ì¤€ìœ¼ë¡œ ì—”íŠ¸ë¦¬IDê°€ ë§¤ì¹­ëœ kcndicì— ëŒ€í•´,\n",
    "    [ì—”íŠ¸ë¦¬ID + ëœ»í’€ì´] ê¸°ë°˜ìœ¼ë¡œ ëœ»í’€ì´ID ë³‘í•©\n",
    "\n",
    "    ë³‘í•© ê¸°ì¤€:\n",
    "    - kcndicì˜ 'ì—”íŠ¸ë¦¬ID'ì™€ 'ëœ»í’€ì´'ê°€ ëª¨ë‘ ì¡´ì¬í•´ì•¼ ë³‘í•© ëŒ€ìƒ\n",
    "    - naverì—ì„œ 'ì—”íŠ¸ë¦¬ID'ì™€ 'ëœ»í’€ì´'ê°€ ëª¨ë‘ ìˆëŠ” í–‰ìœ¼ë¡œ ë³‘í•© key ìƒì„±\n",
    "    - ë³‘í•© key: ì—”íŠ¸ë¦¬ID_ëœ»í’€ì´\n",
    "    \"\"\"\n",
    "\n",
    "    # âœ… ëœ»í’€ì´ ì „ì²˜ë¦¬ (í‘œí˜„ ì°¨ì´ ì •ë¦¬)\n",
    "    naver_df['ì„œìˆ í˜•ëœ»í’€ì´_c'] = naver_df['ì„œìˆ í˜•ëœ»í’€ì´'].apply(clean_naver_definition).str.strip()\n",
    "    kcndic_df['ì„œìˆ í˜•ëœ»í’€ì´_c'] = kcndic_df['ì„œìˆ í˜•ëœ»í’€ì´'].apply(clean_kcndic_definition).str.strip()\n",
    "\n",
    "    # ğŸ”‘ naver: ë³‘í•©ìš© key ìƒì„± (ì—”íŠ¸ë¦¬ID, ëœ»í’€ì´ ëª¨ë‘ ìˆëŠ” í–‰ë§Œ)\n",
    "    naver_valid = naver_df[naver_df['ì—”íŠ¸ë¦¬ID_f'].notna() & naver_df['ì„œìˆ í˜•ëœ»í’€ì´_c'].notna()].copy()\n",
    "    naver_valid['key'] = naver_valid['ì—”íŠ¸ë¦¬ID_f'].astype(str) + '_' + naver_valid['ì„œìˆ í˜•ëœ»í’€ì´_c']\n",
    "\n",
    "    meaning_cols = [\n",
    "        'ëœ»í’€ì´ID'\n",
    "    ]\n",
    "    naver_keymap = naver_valid[['key', 'ëœ»í’€ì´ID']].drop_duplicates('key')\n",
    "    # ì¤‘ë³µëœ key ëª‡ê°œ ì œê±°ëë‚˜ \n",
    "\n",
    "    print(f\"ğŸ§© naver ë³‘í•© ëŒ€ìƒ key ìˆ˜: {len(naver_keymap)}\")\n",
    "\n",
    "    # ğŸ”‘ kcndic: ë³‘í•© ëŒ€ìƒ ì¶”ì¶œ (ì—”íŠ¸ë¦¬IDì™€ ëœ»í’€ì´ ëª¨ë‘ ìˆëŠ” í–‰ë§Œ)\n",
    "    kcndic_valid = kcndic_df[kcndic_df['ì—”íŠ¸ë¦¬ID_f'].notna() & kcndic_df['ì„œìˆ í˜•ëœ»í’€ì´_c'].notna()].copy()\n",
    "    kcndic_valid['key'] = kcndic_valid['ì—”íŠ¸ë¦¬ID_f'].astype(str) + '_' + kcndic_valid['ì„œìˆ í˜•ëœ»í’€ì´_c']\n",
    "\n",
    "    print(f\"ğŸ§© kcndic ë³‘í•© ëŒ€ìƒ í–‰ ìˆ˜: {len(kcndic_valid)}\")\n",
    "\n",
    "    # ë³‘í•© í‚¤ êµì§‘í•©, ì°¨ì§‘í•© ì¶”ì¶œ\n",
    "    naver_keys = set(naver_keymap['key'])\n",
    "    kcndic_keys = set(kcndic_valid['key'])\n",
    "\n",
    "    intersection_keys = naver_keys & kcndic_keys\n",
    "    naver_only_keys = naver_keys - kcndic_keys\n",
    "    kcndic_only_keys = kcndic_keys - naver_keys\n",
    "\n",
    "    print(f\"ğŸ” ë³‘í•© key êµì§‘í•© ìˆ˜: {len(intersection_keys)}\")\n",
    "    print(f\"ğŸ“‰ naverì—ë§Œ ì¡´ì¬í•˜ëŠ” key ìˆ˜: {len(naver_only_keys)}\")\n",
    "    print(f\"ğŸ“‰ kcndicì—ë§Œ ì¡´ì¬í•˜ëŠ” key ìˆ˜: {len(kcndic_only_keys)}\")\n",
    "    \n",
    "    # ğŸ”» ì°¨ì§‘í•© ë¡œê·¸: naver only\n",
    "    naver_unmatched = naver_valid[naver_valid['key'].isin(naver_only_keys)][['ìµœì´ˆë“±ë¡ì‹œê°„', 'í¸ì§‘ì', 'ì—”íŠ¸ë¦¬ID_f', 'í‘œì œì–´', 'í‘œì œì–´ì¢…ë¥˜', 'key']]\n",
    "    naver_unmatched.to_excel('../log/250330_key_naver_only.xlsx', index=False)\n",
    "\n",
    "    # ğŸ”» ì°¨ì§‘í•© ë¡œê·¸: kcndic only\n",
    "    kcndic_unmatched = kcndic_valid[kcndic_valid['key'].isin(kcndic_only_keys)][['ìµœì´ˆë“±ë¡ì‹œê°„', 'í¸ì§‘ì', 'ì—”íŠ¸ë¦¬ID_f', 'í‘œì œì–´', 'í‘œì œì–´ì¢…ë¥˜', 'key']]\n",
    "    kcndic_unmatched.to_excel('../log/250330_key_kcndic_only.xlsx', index=False)\n",
    "\n",
    "    # print(kcndic_valid.loc[kcndic_valid['key'].notna(), ['ëœ»í’€ì´ID']].head(10))\n",
    "    # print(naver_keymap.loc[naver_keymap['key'].notna(), ['ëœ»í’€ì´ID']].head(10))\n",
    "    print(naver_keymap['ëœ»í’€ì´ID'].isna().sum())\n",
    "\n",
    "    # ë³‘í•©\n",
    "    merged = pd.merge(kcndic_valid, naver_keymap, on='key', how='left', suffixes=('', '_N'))\n",
    "    print(f\"âœ… ë³‘í•© ì™„ë£Œ: ë§¤ì¹­ëœ ëœ»í’€ì´ID ìˆ˜: {merged['ëœ»í’€ì´ID'].notna().sum()} / {len(merged)}\")\n",
    "\n",
    "    # # ì›ë˜ kcndic_dfì— ëœ»í’€ì´ID ì—…ë°ì´íŠ¸\n",
    "    # updated_df = kcndic_df.copy()\n",
    "    # updated_df.loc[merged.index, 'ëœ»í’€ì´ID'] = merged['ëœ»í’€ì´ID']\n",
    "    \n",
    "    # ğŸ›  key ê¸°ë°˜ ë³‘í•© ê²°ê³¼ ë°˜ì˜\n",
    "    merge_dict = merged.set_index('key')['ëœ»í’€ì´ID'].dropna().to_dict()\n",
    "    updated_df = kcndic_df.copy()\n",
    "    updated_df['key'] = updated_df['ì—”íŠ¸ë¦¬ID_f'].astype(str) + '_' + updated_df['ì„œìˆ í˜•ëœ»í’€ì´_c']\n",
    "    updated_df['ëœ»í’€ì´ID'] = updated_df['key'].map(merge_dict)\n",
    "    print(\"ğŸ§© merge_dict keys:\", list(merge_dict.keys())[:5])\n",
    "    print(\"ğŸ§© updated_df keys:\", list(updated_df['key'].dropna())[:5])\n",
    "\n",
    "    return updated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© naver ë³‘í•© ëŒ€ìƒ key ìˆ˜: 43941\n",
      "ğŸ§© kcndic ë³‘í•© ëŒ€ìƒ í–‰ ìˆ˜: 41596\n",
      "ğŸ” ë³‘í•© key êµì§‘í•© ìˆ˜: 39609\n",
      "ğŸ“‰ naverì—ë§Œ ì¡´ì¬í•˜ëŠ” key ìˆ˜: 4332\n",
      "ğŸ“‰ kcndicì—ë§Œ ì¡´ì¬í•˜ëŠ” key ìˆ˜: 1986\n",
      "2239\n",
      "âœ… ë³‘í•© ì™„ë£Œ: ë§¤ì¹­ëœ ëœ»í’€ì´ID ìˆ˜: 0 / 41596\n",
      "ğŸ§© merge_dict keys: []\n",
      "ğŸ§© updated_df keys: ['df66ab58126047a1bcf2da7f73a8fb87_{\"ko\":\"ì–´ë–¤ ì¥ì†Œë‚˜ ë¬¼ê±´ì˜ ë°”ê¹¥ ê²½ê³„ì— ê°€ê¹Œìš´ ë¶€ë¶„.\"}', '2eaf13776c404e12857be56595dcedc2_{\"ko\":\"í–‰ìœ„ì˜ ì£¼ì²´ì„ì„ ë‚˜íƒ€ë‚´ëŠ” ë§.\"}', '2eaf13776c404e12857be56595dcedc2_{\"ko\":\"ìƒíƒœ ë³€í™”ì˜ ëŒ€ìƒì„ì„ ë‚˜íƒ€ë‚´ëŠ” ë§.\"}', '2eaf13776c404e12857be56595dcedc2_{\"ko\":\"ìƒíƒœë‚˜ ì„±ì§ˆì˜ ëŒ€ìƒì„ì„ ë‚˜íƒ€ë‚´ëŠ” ë§.\"}', '2eaf13776c404e12857be56595dcedc2_{\"ko\":\"<grammar>(â€˜ë˜ë‹¤â€™, â€˜ì•„ë‹ˆë‹¤â€™ì™€ í•¨ê»˜ ì“°ì—¬)</grammar> ë°”ë€Œê±°ë‚˜ ë¶€ì •ë˜ëŠ” ëŒ€ìƒì„ì„ ë‚˜íƒ€ë‚´ëŠ” ë§.\"}']\n"
     ]
    }
   ],
   "source": [
    "# ã„´ã„´\n",
    "result3 = meanId_merge_by_entryID_meaning(naver, result2)\n",
    "# 1. ë¶„ëª… naverì—ë§Œ ì¡´ì¬í•˜ëŠ” í‚¤ ìˆ˜ì—ëŠ” ì‚­ì œë‹¨ì–´..ì¸ë° ì‚­ì œì²˜ë¦¬ ì•ˆëœìƒ‰íˆë“¤ ì˜ ëœ»í’€ì´ì–´ì©Œêµ¬ê°€ ìˆì„ê±°ë€ ë§ì´ì§€ ì´ê±° ëœ»í’€ì´ ì‹œì‘í•˜ê¸° ì „ ì „ì²˜ë¦¬ì—ì„œ ì§€ìš°ê³ ì‹¶ìœ¼ë©´ ì–´ë–»ê²Œ í•˜ì§€? \n",
    "# => ì•„ë§ë‹¤ ã…ã…Š !!!!!!! ì´ê±°ì´ê±° ë‚´ê°€ ì•„ì§ ohwë§Œ í•´ì„œ ì†ë‹´ìˆ™ì–´ê°€ ì—†êµ¬ë‚˜??? í•¨ë¶€ë¡œ ì§€ìš°ë©´ ì•ˆë˜ë„¤! ì†ë‹´ìˆ™ì–´ë¥¼ .. ... ê°€ì ¸ì™€ì•¼ê² ë‹¤.. ìœ¼ì•™ ... ë¬¼ë¡ ë‹¹ì—°íˆ ì‚­ì œë‹¨ì–´ë„ìˆìŒ.. \n",
    "# 2. ì—”íŠ¸ë¦¬ID flll ì‘ì—… ì œëŒ€ë¡œ ëëŠ”ì§€ ã…ˆã„´ìˆ˜ìƒí•œë°? ì´ê±° ê²€ì‚¬í•´ë´ì•¼ë¼;; ë¡œê·¸ë½‘ì..í‘ => ì½”ë“œ ë°”ê¾¸ë‹ˆ ì œëŒ€ë¡œ ë˜ë„¤! ì‹œê°„ì€ ì¢€ ê¸¸ì–´ì¡Œë‹¤ë§Œ.. \n",
    "# 3. ê·¸ë¦¬ê³  ëŒ€ì¶©ë³´ë‹ˆ ëœ»ì´ ìˆ˜ì •ëœ ë…€ì„ì´ ì—­ì‹œ ìˆë„¹... ì¼ë‹¨ 100í¼ì¼ì¹˜ì¸ì• ë“¤ ë§¤ì¹­í•˜ê³ , ë§¤ì¹­ ì‹¤íŒ¨í•œë†ˆë“¤ë§Œ ëŒ€ìƒìœ¼ë¡œ ë­ ì• 8ê¸€ìê¸°ì¤€í•˜ê±°ë‚˜, ì•„ë‹˜ ì¼ì¹˜ìœ¨ 80í¼ ì´ëŸ°ê±¸ë¡œ í•˜ê±°ë‚˜.. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã„±ã„±\n",
    "def meanId_merge_by_entryID_meaning2(naver_df, kcndic_df):\n",
    "    \"\"\"\n",
    "    [í‘œì œì–´] ê¸°ì¤€ìœ¼ë¡œ ì—”íŠ¸ë¦¬IDê°€ ë§¤ì¹­ëœ kcndicì— ëŒ€í•´,\n",
    "    [ì—”íŠ¸ë¦¬ID + ëœ»í’€ì´] ê¸°ë°˜ìœ¼ë¡œ ëœ»í’€ì´ID ë³‘í•©\n",
    "\n",
    "    ë³‘í•© ê¸°ì¤€:\n",
    "    - kcndicì˜ 'ì—”íŠ¸ë¦¬ID'ì™€ 'ëœ»í’€ì´'ê°€ ëª¨ë‘ ì¡´ì¬í•´ì•¼ ë³‘í•© ëŒ€ìƒ\n",
    "    - naverì—ì„œ 'ì—”íŠ¸ë¦¬ID'ì™€ 'ëœ»í’€ì´'ê°€ ëª¨ë‘ ìˆëŠ” í–‰ìœ¼ë¡œ ë³‘í•© key ìƒì„±\n",
    "    - ë³‘í•© key: ì—”íŠ¸ë¦¬ID_ëœ»í’€ì´\n",
    "    \"\"\"\n",
    "\n",
    "    # âœ… ëœ»í’€ì´ ì „ì²˜ë¦¬ (í‘œí˜„ ì°¨ì´ ì •ë¦¬)\n",
    "    naver_df['ì„œìˆ í˜•ëœ»í’€ì´_c'] = naver_df['ì„œìˆ í˜•ëœ»í’€ì´'].apply(clean_naver_definition).str.strip()\n",
    "    kcndic_df['ì„œìˆ í˜•ëœ»í’€ì´_c'] = kcndic_df['ì„œìˆ í˜•ëœ»í’€ì´'].apply(clean_kcndic_definition).str.strip()\n",
    "\n",
    "    def make_key(df, source):\n",
    "        df['key_m'] = None\n",
    "        # ìœ íš¨í•œ í–‰: [ì—”íŠ¸ë¦¬ID_f], [ì„œìˆ í˜•ëœ»í’€ì´_c] ëª¨ë‘ NaNì•„ë‹˜\n",
    "        valid_mask = df['ì—”íŠ¸ë¦¬ID_f'].notna() & df['ì„œìˆ í˜•ëœ»í’€ì´_c'].notna()\n",
    "        df.loc[valid_mask, 'key_m'] = df.loc[valid_mask, 'ì—”íŠ¸ë¦¬ID_f'].astype(str) + '_' + df.loc[valid_mask, 'ì„œìˆ í˜•ëœ»í’€ì´_c']\n",
    "        return df\n",
    "    \n",
    "    # í‚¤ ìƒì„±\n",
    "    naver_df = make_key(naver_df, 'naver')\n",
    "    kcndic_df = make_key(kcndic_df, 'kcndic')\n",
    "\n",
    "    # ê³ ìœ  keyë§Œ ì¶”ì¶œ\n",
    "    meaning_cols = [\n",
    "        'ëœ»í’€ì´ID'\n",
    "    ]\n",
    "    naver_sub = naver_df[naver_df['key_m'].notna()][['key_m'] + meaning_cols].drop_duplicates('key_m')\n",
    "\n",
    "    # chunk ë³‘í•©\n",
    "    chunks = []\n",
    "    unmatched_rows = []\n",
    "    total = len(kcndic_df)\n",
    "    chunk_size = 50000\n",
    "    \n",
    "    for i in range(0, total, chunk_size):\n",
    "        print(i,\"ë²ˆì§¸ë°˜ë³µ\")\n",
    "        kc_chunk = kcndic_df.iloc[i:i+chunk_size].copy()\n",
    "\n",
    "        merged_chunk = pd.merge(kc_chunk, naver_sub, on='key_m', how='left', suffixes=('', '_N'))\n",
    "\n",
    "        for col in meaning_cols:\n",
    "            merged_chunk[col] = merged_chunk[col + '_N']\n",
    "\n",
    "        merged_chunk.drop(columns=[col + '_N' for col in meaning_cols], inplace=True)\n",
    "\n",
    "        chunks.append(merged_chunk)\n",
    "        print(f\"âœ… Chunk {i//chunk_size + 1} ë³‘í•© ì™„ë£Œ\")\n",
    "\n",
    "    # ì „ì²´ ë³‘í•© ê²°ê³¼\n",
    "    merged = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ë²ˆì§¸ë°˜ë³µ\n",
      "âœ… Chunk 1 ë³‘í•© ì™„ë£Œ\n",
      "50000 ë²ˆì§¸ë°˜ë³µ\n",
      "âœ… Chunk 2 ë³‘í•© ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ã„±ã„±\n",
    "result3 = meanId_merge_by_entryID_meaning2(naver, result2)\n",
    "# ì—­ì‹œ ì´ë°©ë²•ì´ ì œëŒ€ë¡œ ë¶™ëŠ”êµ°! ë¡œê·¸ëŠ” ë‹¤ ë‚ ë¼ê°”ì§€ë§Œ....ã…ã…ã… ã… .... í•˜... \n",
    "# ë³´ë‹ˆê¹Œ kcndicìª½ ì„œìˆ í˜•ëœ»í’€ì´ì— <grammar>ì´ ì—†ì–´ì„œ ë§¤ì¹­ ì•ˆë˜ëŠ” ê²½ìš°ê°€ ë§ì•„ë³´ì´ëŠ”ë° ë­˜ê¹Œ? í™•ì¸ìš”ë§... \n",
    "\n",
    "# ì•”íŠ¼ ì´ ì½”ë“œë¡œ ë˜‘ê°™ì´ ìƒê¸´ê±° ë‹¤ ì²˜ë¦¬í•˜ë©´, ì²˜ë¦¬ëœê°œìˆ˜, ì•ˆëœê°œìˆ˜ ì¹´ìš´íŠ¸... ì•ˆëœ í–‰ ë²ˆí˜¸? ì •ë³´ë§Œ ë”°ë¡œ returní•˜ì. ê·¸ë¦¬ê³  ì´ì œ 80í¼ì¼ì¹˜ìœ¨ ì´ëŸ°ê²ƒë“¤ì„ ë§¤ì¹­í•´ì£¼ì\n",
    "\n",
    "# ê·¸ì „ì— ì†ë‹´ìˆ™ì–´ ì²˜ë¦¬ë¶€í„° ì¶”ê°€í• ê¹Œë´ ~.~ \n",
    "# 250330: ìƒˆì½”ë“œë¡œë§Œë“¦\n",
    "#   - 250324_í•œí•œì¤‘ì˜¤í”ˆí”„ë¡œ, 250328_id_result1.2_ì—”íŠ¸ë¦¬ID_ì œê³µì²˜IDì—†ë˜ì• ë“¤ì‹œë„\n",
    "#     ì–˜ë„¤ ì—´ê¸°!! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3.to_csv('../result/250330_id_result4_ëœ»í’€ì´ID_ë³‘í•©.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            -ã„´ëŒ€_57092.0\n",
       "2         ì„œê¸€íëŠ”ë°_441213.0\n",
       "5          ë°”ê¿” ë§í•˜ë©´_1869.0\n",
       "6          ë§ˆëŠ”êµ¬ë‚˜_440503.0\n",
       "7            ì¶œì‚°_332169.0\n",
       "               ...      \n",
       "137443     êµ­ë¦½ ê³µì›_39172.0\n",
       "137445       ê³ ì ì§€_25214.0\n",
       "137446      ê¼¬ë“¤ê¼¬ë“¤_54625.0\n",
       "137447       ê·œë²”ì„±_43968.0\n",
       "137448      ê³µëª…ì •ëŒ€_28742.0\n",
       "Name: key, Length: 30248, dtype: object"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naver.loc[naver['key'].notna(), 'key'] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
